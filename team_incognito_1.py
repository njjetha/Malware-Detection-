# -*- coding: utf-8 -*-
"""Team_Incognito_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q3JE9GjLOskDmOzL1Yt-c-v8wYttd_Pb
"""

#from google.colab import drive  
#drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,accuracy_score
from sklearn.linear_model import LogisticRegressionCV
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

#train=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Assignment 2/train.csv")
#test=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Assignment 2/test_to_give.csv")

train=pd.read_csv("../input/assignment2/train.csv(1)/train.csv",low_memory=False)
test=pd.read_csv("../input/assignment2/test_to_give.csv/test_to_give.csv",low_memory=False)

"""# Visulaizing

"""

train.info()

train.describe()

train.shape

"""# Data Cleaning """

# Null values that pass this rate
na_threshold = 0.5

# Threshold to remove columns with unbalanced features to their values 
unbalanced_threshold = 0.9

good_cols = list(train.columns)
cnt=0
for col in train.columns:
    
    # Remove columns with high NA rate
    na_rate = train[col].isnull().sum() / train.shape[0]
    
    # Remove columns with high Unbalanced values rate
    unbalanced_rate = train[col].value_counts(normalize=True, dropna=False).values[0]
    
    if na_rate > na_threshold:
        good_cols.remove(col)
    elif unbalanced_rate > unbalanced_threshold:
        print(col)
        cnt=cnt+1
        good_cols.remove(col)

print('Number of coulmns which are unbalanced are :',cnt)

print('Number of columns which has less than 50% null value and less than 90% unbalanced value are :',len(good_cols))

train = train[good_cols]

train.info()

good_cols.remove('HasDetections')

#Updating the train Dataset which has columns with less null values and less unbalanced columns 
test=test[good_cols]

test.info()

categorical_columns = list(train.loc[:, train.dtypes =="object"].columns)
numerical_and_binary_columns = list(train.loc[:, train.dtypes !="object"].columns)
numerical_columns = numerical_and_binary_columns

categorical_columns.remove("MachineIdentifier")

binary_columns = []
for col in (numerical_and_binary_columns):
    if train[col].nunique() == 2:
        binary_columns.append(col)
        numerical_columns.remove(col)

#updated Columns 
print(categorical_columns)
print(binary_columns)
print(numerical_columns)

#creating the submission file that contain MachineIdentifier as the id and HasDetection as the probability
submission = pd.DataFrame({"MachineIdentifier":test['MachineIdentifier']})

#Removing the MachineIdentifier from the train and test dataset
train = train.drop(['MachineIdentifier'], axis=1)
test = test.drop(['MachineIdentifier'], axis=1)

train = train.reset_index(drop=True)

train.mode()

#find the mode of train dataset 
modes = train.mode()
# replacing the NA values with mode of train dataset
for col in train.columns:
    train[col] = np.where(train[col].isnull(), modes[col], train[col])
del modes

#find the mode of test dataset 
modes = test.mode()
# replacing the NA values with mode of train datase
for col in test.columns:
    test[col] = np.where(test[col].isnull(), modes[col], test[col])
del modes

#train dataset we are sepreating the target value and data value 
y = train['HasDetections']
X = train.drop(['HasDetections'], axis=1)

"""
# Splitting Train Data into X_train and X_val"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
import time

# create a 80/20 split of the data 
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)

"""## CatBoost Model"""

from catboost import CatBoostClassifier

clf = CatBoostClassifier(
    iterations=2000, 
    learning_rate=0.1, 
    #loss_function='CrossEntropy'
)


clf.fit(X_train, y_train, 
        cat_features=categorical_columns, 
        eval_set=(X_val, y_val), 
        verbose=False
)

print('CatBoost model is fitted: ' + str(clf.is_fitted()))
print('CatBoost model parameters:')
print(clf.get_params())

predictions = clf.predict(X_val)

print("accuracy_score", accuracy_score(y_val, predictions))

predictions_probas = clf.predict_proba(X_val)
print("roc-auc score for the class 1, from target 'HasDetections' ", roc_auc_score(y_val, predictions_probas[:,1]))

val_cnf_matrix=confusion_matrix(y_val,predictions)
sns.heatmap(val_cnf_matrix, annot=True, fmt='.2f', cmap="BrBG").set_title("Validation")
plt.show()

#completely training the whole train dataset
clf.fit(X, y,cat_features=categorical_columns,verbose=False)

predictions_probas = clf.predict_proba(test)[:,1]

predictions_probas

submission["HasDetections"] = predictions_probas
submission.to_csv("Catboost_ML_Assignment.csv", index=False)
submission.head(20)

"""On submitting "Catboost_ML_Assignment.csv" on  Kaggle we got our highest score of 0.71553 """